{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eaa9a5e-1d1e-4a95-a48d-678697b73e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30122c1-2b35-4ce2-a586-cbaf01fc56fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galkesten/miniconda3/envs/sana/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/galkesten/miniconda3/envs/sana/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a89af691c642ee87f3fd7a4c271298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c9b3702e91425a9bd54bfc96982e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    #\"Efficient-Large-Model/Sana_600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e48849-1ccd-4600-ad91-6b66a6866d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_callback_on_step_end(pipeline, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "    prompt_embeds = callback_kwargs[\"prompt_embeds\"]\n",
    "    negative_prompt_embeds = callback_kwargs[\"negative_prompt_embeds\"]\n",
    "\n",
    "    print(f\"[Step {step}] Timestep: {timestep}\")\n",
    "    print(f\"Latents shape: {latents.shape}\")\n",
    "    # print(f\"Prompt embeds shape: {prompt_embeds.shape}\")\n",
    "    # if negative_prompt_embeds is not None:\n",
    "    #     print(f\"Negative prompt embeds shape: {negative_prompt_embeds.shape}\")\n",
    "    return {\n",
    "        \"latents\": latents,\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"negative_prompt_embeds\": negative_prompt_embeds,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7715323b-d355-4de0-b458-0bbf2adcbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        print(f\"name: {name}\")\n",
    "        print(type(input))\n",
    "        print(len(input))\n",
    "        if isinstance(output, tuple):\n",
    "            for i, o in enumerate(output):\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    print(f\"{module.__class__.__name__} output[{i}] shape: {tuple(o.shape)}\")\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            print(f\"{module.__class__.__name__} output shape: {tuple(output.shape)}\")\n",
    "        else:\n",
    "             print(f\"{module.__class__.__name__}, {type(output)}\")\n",
    "        print(\"\\n\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "\n",
    "# Register hooks\n",
    "\n",
    "hook_handles = []\n",
    "\n",
    "hook_handles.append(pipe.transformer.patch_embed.register_forward_hook(print_shape_hook(\"patch_embed\")))\n",
    "hook_handles.append(pipe.transformer.time_embed.register_forward_hook(print_shape_hook(\"time_embed\")))\n",
    "\n",
    "hook_handles.append(pipe.transformer.caption_projection.register_forward_hook(print_shape_hook(\"caption_projection\")))\n",
    "hook_handles.append(pipe.transformer.caption_norm.register_forward_hook(print_shape_hook(\"caption_norm\")))\n",
    "\n",
    "hook_handles.append(pipe.transformer.transformer_blocks[0].norm1.register_forward_hook(print_shape_hook(\"transformer_blocks.0.norm1\")))\n",
    "hook_handles.append(pipe.transformer.transformer_blocks[0].attn1.register_forward_hook(print_shape_hook(\"transformer_blocks.0.attn1\")))\n",
    "hook_handles.append(pipe.transformer.transformer_blocks[0].norm2.register_forward_hook(print_shape_hook(\"transformer_blocks.0.norm2\")))\n",
    "hook_handles.append(pipe.transformer.transformer_blocks[0].attn2.register_forward_hook(print_shape_hook(\"transformer_blocks.0.attn2\")))\n",
    "hook_handles.append(pipe.transformer.transformer_blocks[0].ff.register_forward_hook(print_shape_hook(\"transformer_blocks.0.ff\")))\n",
    "\n",
    "hook_handles.append(pipe.transformer.norm_out.register_forward_hook(print_shape_hook(\"norm_out\")))\n",
    "hook_handles.append(pipe.transformer.proj_out.register_forward_hook(print_shape_hook(\"proj_out\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9070f5e7-fb9d-4592-addf-bc821a07e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000 prompts.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "# Download only the meta_data.json info file\n",
    "json_path = hf_hub_download(\n",
    "    repo_id=\"playgroundai/MJHQ-30K\",\n",
    "    filename=\"meta_data.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Load the JSON and extract prompts\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "prompts = [info[\"prompt\"] for info in data.values()]\n",
    "print(f\"Loaded {len(prompts)} prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313d331b-727b-4bc2-8dbd-23fdc31a6820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful Jaguar decorated with huichol beads, in the jungle, plants everywhere, DMT colours, ultra realistic , cinematic lighting   v 5',\n",
       " 'a hamster dressed in cia outfit',\n",
       " 'Jesus the shepherd leading the sheep out of the fence in Palestine 2000 years ago',\n",
       " 'beautiful pale pink baby fawn set among pastel pink flowers and soft wisteria, romantic specimen, pinkish white background, by Thomas kinkade Nadja Baxter Anne Stokes Nancy Noel ',\n",
       " 'hyperrealistic portrait of a young woman, Belarus, over the shoulder, brown and long hair, white dress, tree of life, symbol of birth and fertility, shedding light around her, depicted with swan and horse, Nikon D850, 85mm lens, f1.8, intense contrast, vivid colors ',\n",
       " 'A hyper detail painting in richard macneil style of a duck with her ducklings, walking through a field were there are cows grazing ',\n",
       " 'A white horse in a storm of fire above the ocean ',\n",
       " 'tiger cub playing with soccer ball ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sana = prompts[0:8]\n",
    "input_sana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc81d74f-bd81-411d-98a5-970834805789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SanaTransformer2DModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(32, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (time_embed): AdaLayerNormSingle(\n",
       "    (emb): PixArtAlphaCombinedTimestepSizeEmbeddings(\n",
       "      (time_proj): Timesteps()\n",
       "      (timestep_embedder): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=256, out_features=1152, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=1152, out_features=6912, bias=True)\n",
       "  )\n",
       "  (caption_projection): PixArtAlphaTextProjection(\n",
       "    (linear_1): Linear(in_features=2304, out_features=1152, bias=True)\n",
       "    (act_1): GELU(approximate='tanh')\n",
       "    (linear_2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (caption_norm): RMSNorm()\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-27): 28 x SanaTransformerBlock(\n",
       "      (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "        (to_k): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "        (to_v): Linear(in_features=1152, out_features=1152, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_k): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_v): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): GLUMBConv(\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_inverted): Conv2d(1152, 5760, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (conv_depth): Conv2d(5760, 5760, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5760)\n",
       "        (conv_point): Conv2d(2880, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): SanaModulatedNorm(\n",
       "    (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1152, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf4c3f9-5a78-44ea-bf93-ae3ad74713be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2c918e8b264e4ea75bf4875c7c6f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 0] Timestep: 999\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 1] Timestep: 982\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 2] Timestep: 963\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 3] Timestep: 944\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 4] Timestep: 922\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 5] Timestep: 899\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 6] Timestep: 874\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 7] Timestep: 847\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 8] Timestep: 817\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 9] Timestep: 785\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 10] Timestep: 749\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 11] Timestep: 710\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 12] Timestep: 666\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 13] Timestep: 617\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 14] Timestep: 562\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 15] Timestep: 499\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 16] Timestep: 428\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 17] Timestep: 345\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 18] Timestep: 249\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n",
      "name: patch_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "PatchEmbed output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: time_embed\n",
      "<class 'tuple'>\n",
      "1\n",
      "AdaLayerNormSingle output[0] shape: (16, 6912)\n",
      "AdaLayerNormSingle output[1] shape: (16, 1152)\n",
      "\n",
      "\n",
      "name: caption_projection\n",
      "<class 'tuple'>\n",
      "1\n",
      "PixArtAlphaTextProjection output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: caption_norm\n",
      "<class 'tuple'>\n",
      "1\n",
      "RMSNorm output shape: (16, 300, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm1\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn1\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.attn2\n",
      "<class 'tuple'>\n",
      "1\n",
      "Attention output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.norm2\n",
      "<class 'tuple'>\n",
      "1\n",
      "LayerNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: transformer_blocks.0.ff\n",
      "<class 'tuple'>\n",
      "1\n",
      "GLUMBConv output shape: (16, 1152, 32, 32)\n",
      "\n",
      "\n",
      "name: norm_out\n",
      "<class 'tuple'>\n",
      "3\n",
      "SanaModulatedNorm output shape: (16, 1024, 1152)\n",
      "\n",
      "\n",
      "name: proj_out\n",
      "<class 'tuple'>\n",
      "1\n",
      "Linear output shape: (16, 1024, 32)\n",
      "\n",
      "\n",
      "[Step 19] Timestep: 136\n",
      "Latents shape: torch.Size([8, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A red apple floating below a yellow banana on a white background.'\n",
    "images = pipe(\n",
    "    prompt=input_sana,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=5.0,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    "    callback_on_step_end=debug_callback_on_step_end,\n",
    "    callback_on_step_end_tensor_inputs=[\"latents\", \"prompt_embeds\",  \"negative_prompt_embeds\"])[0]\n",
    "\n",
    "#image[0].save(\"sana4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de901977-9f0e-402e-8f3f-e58fc6325a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "for im in images:\n",
    "    i = i+1\n",
    "    im.save(f\"sana{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9925cca-8aa7-48c0-9d66-e2a02add7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b4e94-6907-4737-85af-bc45dc3cf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.sana.pipeline_sana import retrieve_timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c28d14-7ea8-4e26-b816-cb9007cc03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = retrieve_timesteps(pipe.scheduler, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617c49b-ce22-4453-a9b3-5c145c4f1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4c5a6-252d-4c2c-80af-3178e974bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesteps(pipe, timesteps_step_size=5):\n",
    "    timesteps = retrieve_timesteps(pipe.scheduler, 20)[0]\n",
    "    if isinstance(timesteps, list):\n",
    "        timesteps = torch.tensor([int(t) if torch.is_tensor(t) else t for t in timesteps])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, t in enumerate(timesteps):\n",
    "        if i % timesteps_step_size == 0:\n",
    "            results.append(t)\n",
    "        elif i == len(timesteps) - 1:\n",
    "            results.append(t)\n",
    "\n",
    "    return results\n",
    "    \n",
    "get_timesteps(pipe, timesteps_step_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244a0eb-e378-4e86-a323-5bd641f3015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288833b-eccf-4acc-a72d-3359dc3870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe.scheduler.config)  # contains beta/sigma schedule, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e4c6c-6093-4bcf-9683-3c6bcd21df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "H, W = 32, 32  # image dimensions\n",
    "\n",
    "# Horizontal gradient\n",
    "horizontal = np.linspace(0, 1, W)[None, :].repeat(H, axis=0)\n",
    "\n",
    "# Vertical gradient\n",
    "vertical = np.linspace(0, 1, H)[:, None].repeat(W, axis=1)\n",
    "\n",
    "# Gaussian blob\n",
    "x = np.linspace(-1, 1, W)\n",
    "y = np.linspace(-1, 1, H)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "gaussian = np.exp(-(xv**2 + yv**2) / (2 * 0.3**2))  # 2D Gaussian\n",
    "\n",
    "# Horizontal stripes\n",
    "h_stripes = ((np.arange(H) // 8) % 2)[:, None].repeat(W, axis=1)\n",
    "\n",
    "# Vertical stripes\n",
    "v_stripes = ((np.arange(W) // 8) % 2)[None, :].repeat(H, axis=0)\n",
    "\n",
    "# Plot all\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for ax, data, title in zip(\n",
    "    axes,\n",
    "    [horizontal, vertical, gaussian, h_stripes, v_stripes],\n",
    "    [\"Horizontal\", \"Vertical\", \"Gaussian\", \"H Stripes\", \"V Stripes\"]\n",
    "):\n",
    "    ax.imshow(data, cmap=\"viridis\")\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215e1da-99b6-45a4-942a-3b66b1210a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e49252-8345-48bb-aa09-e50926ff48e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57a297-f1f5-4fef-98ea-93d234924d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a263f-ec57-47a9-95b5-f4fda58e22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "kernel_size = 5\n",
    "x = torch.randn(1, 32, 32, 32)\n",
    "conv = nn.Conv2d(32, 1, (kernel_size, kernel_size), stride=1, padding=0)\n",
    "y=conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0f264-d53f-4b4c-af80-7e3fd100e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899b229-970d-4a7e-bb6b-52fb9f6b633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model, layers_step_size=5):\n",
    "    transformer_blocks = model.transformer_blocks\n",
    "    num_layers = len(transformer_blocks)\n",
    "    results = list(range(0, num_layers, layers_step_size))\n",
    "\n",
    "    if (num_layers - 1) not in results:\n",
    "        results.append(num_layers - 1)\n",
    "\n",
    "    return results\n",
    "\n",
    "get_layers(pipe.transformer, layers_step_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e6f76-860e-48fd-be8c-70c0fab44a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
