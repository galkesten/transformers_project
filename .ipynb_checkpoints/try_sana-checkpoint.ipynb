{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eaa9a5e-1d1e-4a95-a48d-678697b73e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30122c1-2b35-4ce2-a586-cbaf01fc56fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17f3b9303804388bcdc153c651b4716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790e700f103d48edb5bbed9009a15ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e48849-1ccd-4600-ad91-6b66a6866d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_callback_on_step_end(pipeline, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "    prompt_embeds = callback_kwargs[\"prompt_embeds\"]\n",
    "    negative_prompt_embeds = callback_kwargs[\"negative_prompt_embeds\"]\n",
    "\n",
    "    print(f\"[Step {step}] Timestep: {timestep}\")\n",
    "    print(f\"Latents shape: {latents.shape}\")\n",
    "    print(f\"Prompt embeds shape: {prompt_embeds.shape}\")\n",
    "    if negative_prompt_embeds is not None:\n",
    "        print(f\"Negative prompt embeds shape: {negative_prompt_embeds.shape}\")\n",
    "    print(prompt_embeds[1][0:10])\n",
    "    print(prompt_embeds[0][0:10])\n",
    "    def count_trailing_repeated_embeddings(embeds):\n",
    "        B, L, D = embeds.shape\n",
    "        counts = []\n",
    "        for i in range(B):\n",
    "            seq = embeds[i]\n",
    "            last = seq[-1]\n",
    "            count = 0\n",
    "            for j in reversed(range(L)):\n",
    "                if torch.allclose(seq[j], last, atol=1e-6):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            counts.append(count)\n",
    "        return counts\n",
    "\n",
    "    prompt_pad_counts = count_trailing_repeated_embeddings(prompt_embeds)\n",
    "    print(\"Padding token counts in prompt:\")\n",
    "    for i, c in enumerate(prompt_pad_counts):\n",
    "        print(f\"  Prompt {i}: {c} padding tokens\")\n",
    "\n",
    "    if negative_prompt_embeds is not None:\n",
    "        neg_pad_counts = count_trailing_repeated_embeddings(negative_prompt_embeds)\n",
    "        print(\"Padding token counts in negative prompt:\")\n",
    "        for i, c in enumerate(neg_pad_counts):\n",
    "            print(f\"  Neg Prompt {i}: {c} padding tokens\")\n",
    "\n",
    "    return {\n",
    "        \"latents\": latents,\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"negative_prompt_embeds\": negative_prompt_embeds,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fc0d67-afbd-4c66-b50d-af987fa78cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1967f1340f4b5787a3978caadfd255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6953ad1e95604b8abe1d91b0a6eea4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import SanaPipeline\n",
    "import torch\n",
    "\n",
    "# Step 1: Load pipeline first (no overrides yet!)\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n",
    "# Step 2: Save original method (after pipeline is loaded)\n",
    "original_encode_prompt = pipe.encode_prompt\n",
    "\n",
    "# Step 3: Define hook\n",
    "def short_hook(self, prompt, do_classifier_free_guidance=True, **kwargs):\n",
    "    neg_prompt = kwargs.get(\"negative_prompt\", \"\")\n",
    "    complex_inst = kwargs.get(\"complex_human_instruction\")\n",
    "\n",
    "    print(f\"[HOOK] Prompt: {prompt}\")\n",
    "    print(f\"[HOOK] Negative prompt: {neg_prompt}\")\n",
    "    print(f\"[HOOK] Complex Human Instruction: {complex_inst}\")\n",
    "\n",
    "    # Call original\n",
    "    result = original_encode_prompt(\n",
    "        prompt,\n",
    "        do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Unpack\n",
    "    prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = result\n",
    "\n",
    "    if prompt_attention_mask is not None:\n",
    "        print(prompt_attention_mask)\n",
    "        print(prompt_attention_mask.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Step 4: Assign the hook only on the pipeline instance\n",
    "import types\n",
    "pipe.encode_prompt = types.MethodType(short_hook, pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c065166-77bd-4e67-99de-c333a9efe210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HOOK] Prompt: A red apple floating below a yellow banana on a white background.\n",
      "[HOOK] Negative prompt: \n",
      "[HOOK] Complex Human Instruction: [\"Given a user prompt, generate an 'Enhanced prompt' that provides detailed visual descriptions suitable for image generation. Evaluate the level of detail in the user prompt:\", '- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, and spatial relationships to create vivid and concrete scenes.', '- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.', 'Here are examples of how to transform or refine prompts:', '- User Prompt: A cat sleeping -> Enhanced: A small, fluffy white cat curled up in a round shape, sleeping peacefully on a warm sunny windowsill, surrounded by pots of blooming red flowers.', '- User Prompt: A busy city street -> Enhanced: A bustling city street scene at dusk, featuring glowing street lamps, a diverse crowd of people in colorful clothing, and a double-decker bus passing by towering glass skyscrapers.', 'Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:', 'User Prompt: ']\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "torch.Size([1, 300])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9198792e838e464a95aad172587f431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] Timestep: 999\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 1] Timestep: 982\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 2] Timestep: 963\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 3] Timestep: 944\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 4] Timestep: 922\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 5] Timestep: 899\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 6] Timestep: 874\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 7] Timestep: 847\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 8] Timestep: 817\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 9] Timestep: 785\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 10] Timestep: 749\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 11] Timestep: 710\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 12] Timestep: 666\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 13] Timestep: 617\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 14] Timestep: 562\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 15] Timestep: 499\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 16] Timestep: 428\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 17] Timestep: 345\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 18] Timestep: 249\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n",
      "[Step 19] Timestep: 136\n",
      "Latents shape: torch.Size([1, 32, 32, 32])\n",
      "Prompt embeds shape: torch.Size([2, 300, 2304])\n",
      "Negative prompt embeds shape: torch.Size([1, 300, 2304])\n",
      "tensor([[-0.1553, -0.0232,  0.3047,  ..., -1.8984, -0.1416, -0.6719],\n",
      "        [-0.6211,  0.1748,  0.2451,  ..., -0.4473, -0.3105,  1.0938],\n",
      "        [ 0.0159,  0.0693,  3.2969,  ..., -2.0781,  1.4453, -0.7227],\n",
      "        ...,\n",
      "        [-0.5820,  1.6484,  1.6641,  ..., -0.4004,  1.1953, -2.0000],\n",
      "        [-1.8047,  2.1406, -1.5156,  ...,  1.5078, -1.3828, -0.9336],\n",
      "        [-1.3594, -0.4590, -1.3594,  ..., -3.8906,  1.1562,  0.0645]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[-0.1562, -0.0226,  0.3066,  ..., -1.9141, -0.1406, -0.6641],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        ...,\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125],\n",
      "        [ 0.6602, -0.4863, -0.8320,  ..., -3.0312, -0.4629, -1.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Padding token counts in prompt:\n",
      "  Prompt 0: 299 padding tokens\n",
      "  Prompt 1: 1 padding tokens\n",
      "Padding token counts in negative prompt:\n",
      "  Neg Prompt 0: 299 padding tokens\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A red apple floating below a yellow banana on a white background.'\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=5.0,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    "    callback_on_step_end=debug_callback_on_step_end,\n",
    "    callback_on_step_end_tensor_inputs=[\"latents\", \"prompt_embeds\",  \"negative_prompt_embeds\"])[0]\n",
    "\n",
    "image[0].save(\"sana4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9925cca-8aa7-48c0-9d66-e2a02add7806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SanaTransformer2DModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(32, 2240, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (time_embed): AdaLayerNormSingle(\n",
       "    (emb): PixArtAlphaCombinedTimestepSizeEmbeddings(\n",
       "      (time_proj): Timesteps()\n",
       "      (timestep_embedder): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=256, out_features=2240, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=2240, out_features=13440, bias=True)\n",
       "  )\n",
       "  (caption_projection): PixArtAlphaTextProjection(\n",
       "    (linear_1): Linear(in_features=2304, out_features=2240, bias=True)\n",
       "    (act_1): GELU(approximate='tanh')\n",
       "    (linear_2): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "  )\n",
       "  (caption_norm): RMSNorm()\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-19): 20 x SanaTransformerBlock(\n",
       "      (norm1): LayerNorm((2240,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=2240, out_features=2240, bias=False)\n",
       "        (to_k): Linear(in_features=2240, out_features=2240, bias=False)\n",
       "        (to_v): Linear(in_features=2240, out_features=2240, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((2240,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "        (to_k): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "        (to_v): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=2240, out_features=2240, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): GLUMBConv(\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_inverted): Conv2d(2240, 11200, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (conv_depth): Conv2d(11200, 11200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11200)\n",
       "        (conv_point): Conv2d(5600, 2240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): SanaModulatedNorm(\n",
       "    (norm): LayerNorm((2240,), eps=1e-06, elementwise_affine=False)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=2240, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053b4e94-6907-4737-85af-bc45dc3cf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.sana.pipeline_sana import retrieve_timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c28d14-7ea8-4e26-b816-cb9007cc03ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([999, 982, 963, 944, 922, 899, 874, 847, 817, 785, 749, 710, 666, 617,\n",
       "         562, 499, 428, 345, 249, 136]),\n",
       " 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_timesteps(pipe.scheduler, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0244a0eb-e378-4e86-a323-5bd641f3015d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPMSolverMultistepScheduler {\n",
       "  \"_class_name\": \"DPMSolverMultistepScheduler\",\n",
       "  \"_diffusers_version\": \"0.34.0.dev0\",\n",
       "  \"algorithm_type\": \"dpmsolver++\",\n",
       "  \"beta_end\": 0.02,\n",
       "  \"beta_schedule\": \"linear\",\n",
       "  \"beta_start\": 0.0001,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"euler_at_final\": false,\n",
       "  \"final_sigmas_type\": \"zero\",\n",
       "  \"flow_shift\": 3.0,\n",
       "  \"lambda_min_clipped\": -Infinity,\n",
       "  \"lower_order_final\": true,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"flow_prediction\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"solver_order\": 2,\n",
       "  \"solver_type\": \"midpoint\",\n",
       "  \"steps_offset\": 0,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"linspace\",\n",
       "  \"trained_betas\": null,\n",
       "  \"use_beta_sigmas\": false,\n",
       "  \"use_exponential_sigmas\": false,\n",
       "  \"use_flow_sigmas\": true,\n",
       "  \"use_karras_sigmas\": false,\n",
       "  \"use_lu_lambdas\": false,\n",
       "  \"variance_type\": null\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b288833b-eccf-4acc-a72d-3359dc3870f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict([('num_train_timesteps', 1000), ('beta_start', 0.0001), ('beta_end', 0.02), ('beta_schedule', 'linear'), ('trained_betas', None), ('solver_order', 2), ('prediction_type', 'flow_prediction'), ('thresholding', False), ('dynamic_thresholding_ratio', 0.995), ('sample_max_value', 1.0), ('algorithm_type', 'dpmsolver++'), ('solver_type', 'midpoint'), ('lower_order_final', True), ('euler_at_final', False), ('use_karras_sigmas', False), ('use_exponential_sigmas', False), ('use_beta_sigmas', False), ('use_lu_lambdas', False), ('use_flow_sigmas', True), ('flow_shift', 3.0), ('final_sigmas_type', 'zero'), ('lambda_min_clipped', -inf), ('variance_type', None), ('timestep_spacing', 'linspace'), ('steps_offset', 0), ('rescale_betas_zero_snr', False), ('_class_name', 'DPMSolverMultistepScheduler'), ('_diffusers_version', '0.34.0.dev0')])\n"
     ]
    }
   ],
   "source": [
    "print(pipe.scheduler.config)  # contains beta/sigma schedule, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b940e448-4c43-4765-a446-5db4ce20b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_channels = pipe.transformer.config.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59229cb-f607-4216-883d-d15bbb262a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipe.transformer.config.out_channels // 2 == latent_channels:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c330b1f5-3be5-48ac-af11-1f5d4f7fe601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transformer.config.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c1c75e-7a4e-44b7-a3c1-8b20bd7267b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A red apple floating below a yellow banana on a white background.\"\n",
    "complex_human_instruction =[\n",
    "            \"Given a user prompt, generate an 'Enhanced prompt' that provides detailed visual descriptions suitable for image generation. Evaluate the level of detail in the user prompt:\",\n",
    "            \"- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, and spatial relationships to create vivid and concrete scenes.\",\n",
    "            \"- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.\",\n",
    "            \"Here are examples of how to transform or refine prompts:\",\n",
    "            \"- User Prompt: A cat sleeping -> Enhanced: A small, fluffy white cat curled up in a round shape, sleeping peacefully on a warm sunny windowsill, surrounded by pots of blooming red flowers.\",\n",
    "            \"- User Prompt: A busy city street -> Enhanced: A bustling city street scene at dusk, featuring glowing street lamps, a diverse crowd of people in colorful clothing, and a double-decker bus passing by towering glass skyscrapers.\",\n",
    "            \"Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:\",\n",
    "            \"User Prompt: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcff186e-0893-4acd-9018-54758aaab551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galkesten/miniconda3/envs/sana/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/galkesten/miniconda3/envs/sana/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c76a7ac191403499e8176823f55066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83145660173940678c964ca31d864539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcd583b-e280-43c0-833f-0216ef7228ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A red apple floating below a yellow banana on a white background.\n",
      "[\"Given a user prompt, generate an 'Enhanced prompt' that provides detailed visual descriptions suitable for image generation. Evaluate the level of detail in the user prompt:\", '- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, and spatial relationships to create vivid and concrete scenes.', '- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.', 'Here are examples of how to transform or refine prompts:', '- User Prompt: A cat sleeping -> Enhanced: A small, fluffy white cat curled up in a round shape, sleeping peacefully on a warm sunny windowsill, surrounded by pots of blooming red flowers.', '- User Prompt: A busy city street -> Enhanced: A bustling city street scene at dusk, featuring glowing street lamps, a diverse crowd of people in colorful clothing, and a double-decker bus passing by towering glass skyscrapers.', 'Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:', 'User Prompt: ']\n"
     ]
    }
   ],
   "source": [
    "# 3. Encode input prompt\n",
    "print(prompt)\n",
    "print(complex_human_instruction)\n",
    "prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = pipe.encode_prompt(\n",
    "    prompt=prompt,\n",
    "    do_classifier_free_guidance=True,\n",
    "    num_images_per_prompt=1,\n",
    "    device=\"cuda\",\n",
    "    prompt_embeds=None,\n",
    "    negative_prompt_embeds=None,\n",
    "    prompt_attention_mask=None,\n",
    "    negative_prompt_attention_mask=None,\n",
    "    clean_caption=False,\n",
    "    max_sequence_length=300,\n",
    "    complex_human_instruction=complex_human_instruction,\n",
    "    lora_scale=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb173ae3-6429-4f55-a73b-7623ce257714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "def _get_gemma_prompt_embeds(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        clean_caption: bool = False,\n",
    "        max_sequence_length: int = 300,\n",
    "        complex_human_instruction: Optional[List[str]] = None,\n",
    "    ):\n",
    "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "\n",
    "        if getattr(pipe, \"tokenizer\", None) is not None:\n",
    "            pipe.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        prompt = pipe._text_preprocessing(prompt, clean_caption=clean_caption)\n",
    "\n",
    "        # prepare complex human instruction\n",
    "        if not complex_human_instruction:\n",
    "            max_length_all = max_sequence_length\n",
    "        else:\n",
    "            chi_prompt = \"\\n\".join(complex_human_instruction)\n",
    "            prompt = [chi_prompt + p for p in prompt]\n",
    "            num_chi_prompt_tokens = len(pipe.tokenizer.encode(chi_prompt))\n",
    "            max_length_all = num_chi_prompt_tokens + max_sequence_length - 2\n",
    "\n",
    "        text_inputs = pipe.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length_all,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        prompt_attention_mask = text_inputs.attention_mask\n",
    "        prompt_attention_mask = prompt_attention_mask.to(device)\n",
    "\n",
    "        prompt_embeds = pipe.text_encoder(text_input_ids.to(device), attention_mask=prompt_attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0].to(dtype=dtype, device=device)\n",
    "\n",
    "        return prompt_embeds, prompt_attention_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt: Union[str, List[str]],\n",
    "    do_classifier_free_guidance: bool = True,\n",
    "    negative_prompt: str = \"\",\n",
    "    num_images_per_prompt: int = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    prompt_embeds: Optional[torch.Tensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "    prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "    negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "    clean_caption: bool = False,\n",
    "    max_sequence_length: int = 300,\n",
    "    complex_human_instruction: Optional[List[str]] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "):\n",
    "    r\"\"\"\n",
    "    Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "    Args:\n",
    "        prompt (`str` or `List[str]`, *optional*):\n",
    "            prompt to be encoded\n",
    "        negative_prompt (`str` or `List[str]`, *optional*):\n",
    "            The prompt not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`\n",
    "            instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`). For\n",
    "            PixArt-Alpha, this should be \"\".\n",
    "        do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n",
    "            whether to use classifier free guidance or not\n",
    "        num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "            number of images that should be generated per prompt\n",
    "        device: (`torch.device`, *optional*):\n",
    "            torch device to place the resulting embeddings on\n",
    "        prompt_embeds (`torch.Tensor`, *optional*):\n",
    "            Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "            provided, text embeddings will be generated from `prompt` input argument.\n",
    "        negative_prompt_embeds (`torch.Tensor`, *optional*):\n",
    "            Pre-generated negative text embeddings. For Sana, it's should be the embeddings of the \"\" string.\n",
    "        clean_caption (`bool`, defaults to `False`):\n",
    "            If `True`, the function will preprocess and clean the provided caption before encoding.\n",
    "        max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.\n",
    "        complex_human_instruction (`list[str]`, defaults to `complex_human_instruction`):\n",
    "            If `complex_human_instruction` is not empty, the function will use the complex Human instruction for\n",
    "            the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = pipe._execution_device\n",
    "\n",
    "    if pipe.text_encoder is not None:\n",
    "        dtype = pipe.text_encoder.dtype\n",
    "    else:\n",
    "        dtype = None\n",
    "\n",
    "    # set lora scale so that monkey patched LoRA\n",
    "    # function of text encoder can correctly access it\n",
    "    if lora_scale is not None and isinstance(pipe, SanaLoraLoaderMixin):\n",
    "        pipe._lora_scale = lora_scale\n",
    "\n",
    "        # dynamically adjust the LoRA scale\n",
    "        if pipe.text_encoder is not None and USE_PEFT_BACKEND:\n",
    "            scale_lora_layers(pipe.text_encoder, lora_scale)\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if getattr(pipe, \"tokenizer\", None) is not None:\n",
    "        pipe.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # See Section 3.1. of the paper.\n",
    "    max_length = max_sequence_length\n",
    "    select_index = [0] + list(range(-max_length + 1, 0))\n",
    "\n",
    "    if prompt_embeds is None:\n",
    "        prompt_embeds, prompt_attention_mask = pipe._get_gemma_prompt_embeds(\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            clean_caption=clean_caption,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            complex_human_instruction=complex_human_instruction,\n",
    "        )\n",
    "        print(prompt_attention_mask)\n",
    "        prompt_embeds = prompt_embeds[:, select_index]\n",
    "        prompt_attention_mask = prompt_attention_mask[:, select_index]\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "    prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)\n",
    "    prompt_attention_mask = prompt_attention_mask.repeat(num_images_per_prompt, 1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        negative_prompt = [negative_prompt] * batch_size if isinstance(negative_prompt, str) else negative_prompt\n",
    "        negative_prompt_embeds, negative_prompt_attention_mask = pipe._get_gemma_prompt_embeds(\n",
    "            prompt=negative_prompt,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            clean_caption=clean_caption,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            complex_human_instruction=False,\n",
    "        )\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        negative_prompt_attention_mask = negative_prompt_attention_mask.view(bs_embed, -1)\n",
    "        negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(num_images_per_prompt, 1)\n",
    "    else:\n",
    "        negative_prompt_embeds = None\n",
    "        negative_prompt_attention_mask = None\n",
    "\n",
    "    return prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a900aa0d-00e0-4f70-b8de-2513746e690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = encode_prompt(\n",
    "    prompt=prompt,\n",
    "    do_classifier_free_guidance=True,\n",
    "    num_images_per_prompt=1,\n",
    "    device=\"cuda\",\n",
    "    prompt_embeds=None,\n",
    "    negative_prompt_embeds=None,\n",
    "    prompt_attention_mask=None,\n",
    "    negative_prompt_attention_mask=None,\n",
    "    clean_caption=False,\n",
    "    max_sequence_length=300,\n",
    "    complex_human_instruction=complex_human_instruction,\n",
    "    lora_scale=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b08c6-506f-4010-a319-49a3006ff3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
