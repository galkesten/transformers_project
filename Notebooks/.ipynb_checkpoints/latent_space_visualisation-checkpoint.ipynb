{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23541ce1-5dc0-438a-a5ee-397b204e63f2",
   "metadata": {},
   "source": [
    "# SANA Latent-Space Visualizer\n",
    "\n",
    "This experiment hooks into the **self-attention**, **cross-attention**, and **FFN** modules of the SANA Transformer to visualize how the latent representations evolve across **transformer layers** and **diffusion timesteps**.\n",
    "\n",
    "For each timestep in the denoising process, the **input and output** of every module in each block is captured, decoded using the VAE, and saved as an image. This helps reveal the internal dynamics of how the model builds up the final generated image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f42cc-dad5-476e-bdf5-8ea8ac963721",
   "metadata": {},
   "source": [
    "\"A banana on the left side and a apple on the right side.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3decff-df7c-489d-b0e6-534a265c739f",
   "metadata": {},
   "source": [
    "\"A tree on the left side and a car on the right side.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d67bf1-b2bc-4e9e-97a4-60e6e2a1e007",
   "metadata": {},
   "source": [
    "\"a flower at the top of the image and a balloon at the bottom of the image.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a5515-fa05-48a2-8309-e65d1d5d8b96",
   "metadata": {},
   "source": [
    "\"a flower at the top, house in the bootom and sky in the right.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab67a7-45c9-4d86-b709-52b8c0bdb9d2",
   "metadata": {},
   "source": [
    "\"a cyberpunk cat with a neon sign that says \"Sana\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f24d4-f810-449a-97f5-6e295e90567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#  SANA latent‑space visualiser  (with attn1/attn2 in+out)\n",
    "# -----------------------------------------------\n",
    "import os, math, torch\n",
    "from PIL import Image\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 1)  load pipeline\n",
    "# -----------------------------\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n",
    "root_dir = \"exp1-sana_latent_vis_flower_house_sky\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2)  bookkeeping\n",
    "# -----------------------------\n",
    "step_idx = {\"t\": -1}\n",
    "pipe.transformer.register_forward_pre_hook(lambda *_: step_idx.__setitem__(\"t\", step_idx[\"t\"] + 1))\n",
    "\n",
    "# -----------------------------\n",
    "# 3)  helper – reshape, decode, save\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def decode_and_save(lat, tag, step, block=None):\n",
    "    if isinstance(lat, tuple):\n",
    "        lat = lat[0]\n",
    "\n",
    "    if lat.ndim == 3:                                  # (B,S,C)\n",
    "        B, S, C = lat.shape\n",
    "        if C == 2240:\n",
    "            lat = pipe.transformer.proj_out(lat)       # (B,S,32)\n",
    "        size = int(math.isqrt(S))\n",
    "        lat = lat.transpose(1, 2).reshape(B, 32, size, size)\n",
    "    else:                                              # (B,C,H,W)\n",
    "        if lat.shape[1] == 2240:\n",
    "            B, _, H, W = lat.shape\n",
    "            lat = lat.permute(0, 2, 3, 1).reshape(B, H * W, 2240)\n",
    "            lat = pipe.transformer.proj_out(lat)\n",
    "            lat = lat.transpose(1, 2).reshape(B, 32, H, W)\n",
    "\n",
    "    lat = lat.float() / pipe.vae.config.scaling_factor\n",
    "    lat = lat.to(pipe.vae.decoder.conv_in.weight.dtype)\n",
    "    rgb = pipe.vae.decode(lat).sample.float()\n",
    "    img = ((rgb[0]*0.5+0.5).clamp(0,1)*255).permute(1,2,0)\\\n",
    "          .cpu().to(torch.uint8).numpy()\n",
    "\n",
    "    sub = os.path.join(root_dir, f\"step_{step:02d}\")\n",
    "    os.makedirs(sub, exist_ok=True)\n",
    "    fn = f\"{tag}.png\" if block is None else f\"block{block:02d}_{tag}.png\"\n",
    "    Image.fromarray(img).save(os.path.join(sub, fn))\n",
    "\n",
    "# -----------------------------\n",
    "# 4)  hooks on every block\n",
    "# -----------------------------\n",
    "for blk_id, blk in enumerate(pipe.transformer.transformer_blocks):\n",
    "\n",
    "    # FFN input / output\n",
    "    blk.ff.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"ff_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.ff.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"ff_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn1  (self‑attention)\n",
    "    blk.attn1.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn1_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn1.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn1_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn2  (cross‑attention)\n",
    "    blk.attn2.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn2_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn2.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn2_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5)  transformer output  (post‑hook)\n",
    "# -----------------------------\n",
    "pipe.transformer.register_forward_hook(\n",
    "    lambda _m, _inp, out:\n",
    "        decode_and_save(out[0] if isinstance(out, tuple) else out,\n",
    "                        \"transformer_out\",\n",
    "                        step_idx[\"t\"])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6)  run diffusion\n",
    "# -----------------------------\n",
    "num_steps = 20\n",
    "pipe.scheduler.set_timesteps(num_steps)\n",
    "\n",
    "prompt    = \"a flower at the top, house in the bootom and sky in the right.\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt              = prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_inference_steps = num_steps,\n",
    "        generator           = generator,\n",
    "    ).images\n",
    "\n",
    "images[0].save(os.path.join(root_dir, \"final.png\"))\n",
    "print(f\"✔ Activations saved under “{root_dir}/”\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7ab8c2-9585-4d17-b9f2-eec466f2be52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059fa28754fb491285830e5ae2a995a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e2268da9d9407c8ab0a97114ff0619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333d0293500f4d69aac8a7f5c692a395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Activations saved under “sana_latent_vis_banana_apple/”\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "#  SANA latent‑space visualiser  (with attn1/attn2 in+out)\n",
    "# -----------------------------------------------\n",
    "import os, math, torch\n",
    "from PIL import Image\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 1)  load pipeline\n",
    "# -----------------------------\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n",
    "root_dir = \"sana_latent_vis_banana_apple\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2)  bookkeeping\n",
    "# -----------------------------\n",
    "step_idx = {\"t\": -1}\n",
    "pipe.transformer.register_forward_pre_hook(lambda *_: step_idx.__setitem__(\"t\", step_idx[\"t\"] + 1))\n",
    "\n",
    "# -----------------------------\n",
    "# 3)  helper – reshape, decode, save\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def decode_and_save(lat, tag, step, block=None):\n",
    "    if isinstance(lat, tuple):\n",
    "        lat = lat[0]\n",
    "\n",
    "    if lat.ndim == 3:                                  # (B,S,C)\n",
    "        B, S, C = lat.shape\n",
    "        if C == 2240:\n",
    "            lat = pipe.transformer.proj_out(lat)       # (B,S,32)\n",
    "        size = int(math.isqrt(S))\n",
    "        lat = lat.transpose(1, 2).reshape(B, 32, size, size)\n",
    "    else:                                              # (B,C,H,W)\n",
    "        if lat.shape[1] == 2240:\n",
    "            B, _, H, W = lat.shape\n",
    "            lat = lat.permute(0, 2, 3, 1).reshape(B, H * W, 2240)\n",
    "            lat = pipe.transformer.proj_out(lat)\n",
    "            lat = lat.transpose(1, 2).reshape(B, 32, H, W)\n",
    "\n",
    "    lat = lat.float() / pipe.vae.config.scaling_factor\n",
    "    lat = lat.to(pipe.vae.decoder.conv_in.weight.dtype)\n",
    "    rgb = pipe.vae.decode(lat).sample.float()\n",
    "    img = ((rgb[0]*0.5+0.5).clamp(0,1)*255).permute(1,2,0)\\\n",
    "          .cpu().to(torch.uint8).numpy()\n",
    "\n",
    "    sub = os.path.join(root_dir, f\"step_{step:02d}\")\n",
    "    os.makedirs(sub, exist_ok=True)\n",
    "    fn = f\"{tag}.png\" if block is None else f\"block{block:02d}_{tag}.png\"\n",
    "    Image.fromarray(img).save(os.path.join(sub, fn))\n",
    "\n",
    "# -----------------------------\n",
    "# 4)  hooks on every block\n",
    "# -----------------------------\n",
    "for blk_id, blk in enumerate(pipe.transformer.transformer_blocks):\n",
    "\n",
    "    # FFN input / output\n",
    "    blk.ff.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"ff_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.ff.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"ff_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn1  (self‑attention)\n",
    "    blk.attn1.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn1_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn1.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn1_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn2  (cross‑attention)\n",
    "    blk.attn2.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn2_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn2.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn2_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5)  transformer output  (post‑hook)\n",
    "# -----------------------------\n",
    "pipe.transformer.register_forward_hook(\n",
    "    lambda _m, _inp, out:\n",
    "        decode_and_save(out[0] if isinstance(out, tuple) else out,\n",
    "                        \"transformer_out\",\n",
    "                        step_idx[\"t\"])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6)  run diffusion\n",
    "# -----------------------------\n",
    "num_steps = 20\n",
    "pipe.scheduler.set_timesteps(num_steps)\n",
    "\n",
    "prompt    = \"A banana on the left side and an apple on the right side.\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt              = prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_inference_steps = num_steps,\n",
    "        generator           = generator,\n",
    "    ).images\n",
    "\n",
    "images[0].save(os.path.join(root_dir, \"final.png\"))\n",
    "print(f\"✔ Activations saved under “{root_dir}/”\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25741333-6a9c-420e-b032-026580871445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8f6e72f30e4f37a828ac51519cf901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07c68a88e604d838107fbc4220bf04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322719a2ca9f4b71a6ed0a67bc51c345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Activations saved under “sana_latent_vis_flower_ballo/”\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "#  SANA latent‑space visualiser  (with attn1/attn2 in+out)\n",
    "# -----------------------------------------------\n",
    "import os, math, torch\n",
    "from PIL import Image\n",
    "from diffusers import SanaPipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 1)  load pipeline\n",
    "# -----------------------------\n",
    "pipe = SanaPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_1600M_1024px_diffusers\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.vae.to(torch.bfloat16)\n",
    "pipe.text_encoder.to(torch.bfloat16)\n",
    "\n",
    "root_dir = \"sana_latent_vis_flower_ballo\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2)  bookkeeping\n",
    "# -----------------------------\n",
    "step_idx = {\"t\": -1}\n",
    "pipe.transformer.register_forward_pre_hook(lambda *_: step_idx.__setitem__(\"t\", step_idx[\"t\"] + 1))\n",
    "\n",
    "# -----------------------------\n",
    "# 3)  helper – reshape, decode, save\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def decode_and_save(lat, tag, step, block=None):\n",
    "    if isinstance(lat, tuple):\n",
    "        lat = lat[0]\n",
    "\n",
    "    if lat.ndim == 3:                                  # (B,S,C)\n",
    "        B, S, C = lat.shape\n",
    "        if C == 2240:\n",
    "            lat = pipe.transformer.proj_out(lat)       # (B,S,32)\n",
    "        size = int(math.isqrt(S))\n",
    "        lat = lat.transpose(1, 2).reshape(B, 32, size, size)\n",
    "    else:                                              # (B,C,H,W)\n",
    "        if lat.shape[1] == 2240:\n",
    "            B, _, H, W = lat.shape\n",
    "            lat = lat.permute(0, 2, 3, 1).reshape(B, H * W, 2240)\n",
    "            lat = pipe.transformer.proj_out(lat)\n",
    "            lat = lat.transpose(1, 2).reshape(B, 32, H, W)\n",
    "\n",
    "    lat = lat.float() / pipe.vae.config.scaling_factor\n",
    "    lat = lat.to(pipe.vae.decoder.conv_in.weight.dtype)\n",
    "    rgb = pipe.vae.decode(lat).sample.float()\n",
    "    img = ((rgb[0]*0.5+0.5).clamp(0,1)*255).permute(1,2,0)\\\n",
    "          .cpu().to(torch.uint8).numpy()\n",
    "\n",
    "    sub = os.path.join(root_dir, f\"step_{step:02d}\")\n",
    "    os.makedirs(sub, exist_ok=True)\n",
    "    fn = f\"{tag}.png\" if block is None else f\"block{block:02d}_{tag}.png\"\n",
    "    Image.fromarray(img).save(os.path.join(sub, fn))\n",
    "\n",
    "# -----------------------------\n",
    "# 4)  hooks on every block\n",
    "# -----------------------------\n",
    "for blk_id, blk in enumerate(pipe.transformer.transformer_blocks):\n",
    "\n",
    "    # FFN input / output\n",
    "    blk.ff.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"ff_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.ff.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"ff_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn1  (self‑attention)\n",
    "    blk.attn1.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn1_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn1.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn1_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "    # attn2  (cross‑attention)\n",
    "    blk.attn2.register_forward_pre_hook(\n",
    "        lambda m, inp, idx=blk_id:\n",
    "            decode_and_save(inp[0], \"attn2_in\",  step_idx[\"t\"], idx)\n",
    "    )\n",
    "    blk.attn2.register_forward_hook(\n",
    "        lambda m, inp, out, idx=blk_id:\n",
    "            decode_and_save(out,    \"attn2_out\", step_idx[\"t\"], idx)\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5)  transformer output  (post‑hook)\n",
    "# -----------------------------\n",
    "pipe.transformer.register_forward_hook(\n",
    "    lambda _m, _inp, out:\n",
    "        decode_and_save(out[0] if isinstance(out, tuple) else out,\n",
    "                        \"transformer_out\",\n",
    "                        step_idx[\"t\"])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6)  run diffusion\n",
    "# -----------------------------\n",
    "num_steps = 20\n",
    "pipe.scheduler.set_timesteps(num_steps)\n",
    "\n",
    "prompt    = \"a flower at the top of the image and a balloon at the bottom of the image.\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt              = prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_inference_steps = num_steps,\n",
    "        generator           = generator,\n",
    "    ).images\n",
    "\n",
    "images[0].save(os.path.join(root_dir, \"final.png\"))\n",
    "print(f\"✔ Activations saved under “{root_dir}/”\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6b400-a98a-47c4-a0fb-3e58d7bd67d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
